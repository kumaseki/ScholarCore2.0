# src/drivers/arxiv.py
import arxiv
import logging
from datetime import datetime, timedelta, timezone
from typing import List, Dict, Any
from tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception_type

from src.core.config import GlobalConfig
from src.core.exceptions import FetchError

logger = logging.getLogger("driver.arxiv")

class ArxivDriver:
    def __init__(self):
        self.config = GlobalConfig
        self.safety_limit = 3000
        self.client_settings = {
            "page_size": 100,
            "delay_seconds": 3.0,
            "num_retries": 5
        }

    @retry(
        retry=retry_if_exception_type(Exception), # æ•è·æ‰€æœ‰å¼‚å¸¸è¿›è¡Œé‡è¯•
        stop=stop_after_attempt(3),
        wait=wait_exponential(multiplier=1, min=4, max=10)
    )
    def _fetch_from_client(self, search_obj):
        """
        å—ä¿æŠ¤çš„åŸå­æ“ä½œï¼šè¿æ¥ Arxiv å¹¶è·å–ç”Ÿæˆå™¨ã€‚
        æ³¨æ„ï¼šArxiv æ˜¯ Lazy Loadï¼Œè¿™é‡Œåªæ˜¯å»ºç«‹äº†è¿æ¥æ„å›¾ï¼ŒçœŸæ­£çš„ç½‘ç»œè¯·æ±‚å‘ç”Ÿåœ¨è¿­ä»£æ—¶ã€‚
        ä¸ºäº†ç¡®ä¿ Retry ç”Ÿæ•ˆï¼Œæˆ‘ä»¬åœ¨è¿™é‡Œå¼ºåˆ¶è½¬æ¢æˆ list (è™½ç„¶è¿™ä¼šæ¶ˆè€—å†…å­˜ï¼Œä½†å¯¹äº daily ä»»åŠ¡æ˜¯å®‰å…¨çš„)ã€‚
        """
        logger.debug(f"ğŸ”Œ Connecting to Arxiv API...")
        client = arxiv.Client(**self.client_settings)
        # å¼ºåˆ¶æ¶ˆè€—ç”Ÿæˆå™¨ï¼Œè§¦å‘ç½‘ç»œè¯·æ±‚ï¼Œä»¥ä¾¿ catch å¼‚å¸¸
        return list(client.results(search_obj))

    def search(self, query: str, days_back: int = 1, limit: int = None) -> List[Dict[str, Any]]:
        logger.info(f"ğŸ” Searching Arxiv: query='{query}', days_back={days_back}, limit={limit}")
        
        cutoff_date = datetime.now(timezone.utc) - timedelta(days=days_back)

        # æ ¸å¿ƒé€»è¾‘ï¼šå¦‚æœæœ‰ limitï¼Œå°±ç”¨ limitï¼›å¦åˆ™ç”¨ safety_limit
        # è¿™èƒ½é˜²æ­¢æµ‹è¯•æ—¶ä¸‹è½½å‡ åƒç¯‡
        actual_max = limit if limit else self.safety_limit

        search_obj = arxiv.Search(
            query=query,
            max_results=actual_max, # <--- è¿™é‡Œç”Ÿæ•ˆï¼
            sort_by=arxiv.SortCriterion.SubmittedDate,
            sort_order=arxiv.SortOrder.Descending
        )

        try:
            # è°ƒç”¨å—ä¿æŠ¤çš„æ–¹æ³•
            all_results = self._fetch_from_client(search_obj)
            
            clean_results = []
            for result in all_results:
                # æ—¶é—´ç†”æ–­
                if result.published < cutoff_date:
                    logger.info(f"ğŸ›‘ Reached cutoff date ({result.published.date()}), stopping.")
                    break
                
                paper_meta = {
                    "title": result.title.replace("\n", " ").strip(),
                    "authors": [a.name for a in result.authors],
                    "summary": result.summary.replace("\n", " ").strip(),
                    "published_date": result.published.isoformat(),
                    "arxiv_url": result.entry_id,
                    "pdf_url": result.pdf_url,
                    "categories": result.categories,
                    "journal_ref": result.journal_ref or "N/A"
                }
                clean_results.append(paper_meta)

            logger.info(f"âœ… Fetched {len(clean_results)} papers from Arxiv.")
            return clean_results

        except Exception as e:
            logger.error(f"ğŸ”¥ Arxiv Search Failed after retries: {e}")
            raise FetchError(
                message="Arxiv API unavailable",
                resource_url="arxiv_api",
                details={"query": query, "error": str(e)}
            )